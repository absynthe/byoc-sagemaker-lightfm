{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom inference container\n",
    "1. [Part 1: Packaging your code for inference with Amazon SageMaker](#Part-1:-Packaging-your-code-for-inference-with-Amazon-SageMaker)\n",
    "    1. [How Amazon SageMaker runs your Docker container during hosting](#How-Amazon-SageMaker-runs-your-Docker-container-during-hosting)\n",
    "    1. [The parts of the sample container](#The-parts-of-the-sample-inference-container)\n",
    "1. [Part 2: Building and registering the container](#Part-2:-Building-and-registering-the-container)\n",
    "1. [Part 3: Use the container for inference in Amazon SageMaker](#Part-3:-Use-the-container-for-inference-in-Amazon-SageMaker)\n",
    "  1. [Import model into hosting](#Import-model-into-hosting)\n",
    "  1. [Create endpoint configuration](#Create-endpoint-configuration) \n",
    "  1. [Create endpoint](#Create-endpoint)   \n",
    "  1. [Invoke model](#Invoke-model)     \n",
    "1. [(Optional) cleanup](#(Optional)-cleanup)  \n",
    "\n",
    "## Part 1: Packaging your code for inference with Amazon SageMaker\n",
    "\n",
    "### How Amazon SageMaker runs your Docker container during hosting\n",
    "\n",
    "Because you can run the same image in training or hosting, Amazon SageMaker runs your container with the argument `train` or `serve`. How your container processes this argument depends on the container. All SageMaker framework containers already cover this requirement and will trigger your defined training algorithm and inference code.\n",
    "\n",
    "* If you specify a program as an `ENTRYPOINT` in the Dockerfile, that program will be run at startup and its first argument will be `train` or `serve`. The program can then look at that argument and decide what to do.\n",
    "\n",
    "#### Running your container during hosting\n",
    "\n",
    "Hosting has a very different model than training because hosting is reponding to inference requests that come in via HTTP. \n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` receives `GET` requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these are passed in as well. \n",
    "\n",
    "If you are using the same container image for both training and serving the model, it will have the model files in the same place that they were written to during training:\n",
    "\n",
    "    /opt/ml\n",
    "    `-- model\n",
    "        `-- <model files>\n",
    "        \n",
    "Alternatively, if you are using separate containers for training and inference, when the inference container is spun up, the model files will be copied from the S3 location that the training container outputted them to. \n",
    "\n",
    "### The parts of the sample inference container\n",
    "\n",
    "In order to build a production grade inference server into the container, we use the following stack to make the implementer's job simple:\n",
    "\n",
    "![The Inference Stack](stack.png)\n",
    "\n",
    "1. __[nginx][nginx]__ is a light-weight layer that handles the incoming HTTP requests and manages the I/O in and out of the container efficiently.\n",
    "2. __[gunicorn][gunicorn]__ is a WSGI pre-forking worker server that runs multiple copies of your application and load balances between them.\n",
    "3. __[flask][flask]__ is a simple web framework used in the inference app that you write. It lets you respond to call on the `/ping` and `/invocations` endpoints without having to write much code.\n",
    "\n",
    "The `inference_container` directory has all the components you need to extend the inference logic of the SageMaker scikit-learn container:\n",
    "\n",
    "    .\n",
    "    |-- Dockerfile\n",
    "        |-- light_fm   \n",
    "            |-- nginx.conf\n",
    "            |-- predictor.py\n",
    "            |-- wsgi.py    \n",
    "            |-- serve\n",
    "\n",
    "Let's discuss each of these in turn:\n",
    "\n",
    "* __`Dockerfile`__ The _Dockerfile_ describes how the image is built and what it contains. It is a recipe for your container and gives you tremendous flexibility to construct almost any execution environment you can imagine. Here. we use the Dockerfile to describe a pretty standard python science stack and the simple scripts that we're going to add to it. See the [Dockerfile reference][dockerfile] for what's possible here.\n",
    "* __`serve`__: The wrapper that starts the inference server. In most cases, you can use this file as-is.\n",
    "* __`wsgi.py`__: The start up shell for the individual server workers. This only needs to be changed if you changed where predictor.py is located or is named.\n",
    "* __`predictor.py`__: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\n",
    "* __`nginx.conf`__: The configuration for the nginx master server that manages the multiple workers.\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "When you create an inference server, you can control some of Gunicorn's options via environment variables. These\n",
    "can be supplied as part of the CreateModel API call.\n",
    "\n",
    "    Parameter                Environment Variable              Default Value\n",
    "    ---------                --------------------              -------------\n",
    "    number of workers        MODEL_SERVER_WORKERS              the number of CPU cores\n",
    "    timeout                  MODEL_SERVER_TIMEOUT              60 seconds\n",
    "\n",
    "\n",
    "[skl]: http://scikit-learn.org \"scikit-learn Home Page\"\n",
    "[dockerfile]: https://docs.docker.com/engine/reference/builder/ \"The official Dockerfile reference guide\"\n",
    "[ecr]: https://aws.amazon.com/ecr/ \"ECR Home Page\"\n",
    "[nginx]: http://nginx.org/\n",
    "[gunicorn]: http://gunicorn.org/\n",
    "[flask]: http://flask.pocoo.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building and registering the container\n",
    "\n",
    "Just like with the training container, we are going to use the [Amazon SageMaker Studio Image Build new CLI](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/).\n",
    "\n",
    "Open a terminal window and run the following command:\n",
    "```\n",
    "cd ~/inference_container\n",
    "sm-docker build . --repository lightfm-inference:1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Use the container for inference in Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "def get_container_uri(ecr_repository, tag):\n",
    "    account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    uri_suffix = 'amazonaws.com'\n",
    "    if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "        uri_suffix = 'amazonaws.com.cn'\n",
    "\n",
    "    return '{}.dkr.ecr.{}.{}/{}:{}'.format(account_id, region, uri_suffix, ecr_repository, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model into hosting\n",
    "\n",
    "When creating the Model entity for endpoints, the container's ModelDataUrl is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.\n",
    "\n",
    "The Mode of container is specified as MultiModel to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role()\n",
    "client = boto3.client(service_name='sagemaker')\n",
    "\n",
    "byoc_image_uri = get_container_uri('lightfm-inference','1.0')\n",
    "model_url = 's3://sagemaker-us-east-1-718026778991/light-fm-custom-container-train-job-2021-04-26-10-26-53-215/output/model.tar.gz'\n",
    "model_name = 'Demo-LightFM-Inference-Model'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "container = {\n",
    "    'Image': byoc_image_uri,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'SingleModel'\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'DEMO-LightFM-EndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-LightFMEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke model\n",
    "\n",
    "Now we invoke the model that we uploaded to S3 previously in the training step. \n",
    "\n",
    "The first invocation of a model may be slow, since behind the scenes, SageMaker is downloading the model artifacts from S3 to the instance and loading it into the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change input to CSV\n",
    "\n",
    "%%time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "runtime_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "data = np.array([3, 42, 500])\n",
    "payload = json.dumps(data.tolist())\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "#    TargetModel='resnet_18.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) cleanup\n",
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account, but you can also run the code below to easily clean up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)\n",
    "client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
