{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom inference container\n",
    "1. [Part 1: Packaging your code for inference with Amazon SageMaker](#Part-1:-Packaging-your-code-for-inference-with-Amazon-SageMaker)\n",
    "    1. [How Amazon SageMaker runs your Docker container during hosting](#How-Amazon-SageMaker-runs-your-Docker-container-during-hosting)\n",
    "    1. [The parts of the sample container](#The-parts-of-the-sample-inference-container)\n",
    "       1. [Creating an inference handler](#Creating-an-inference-handler)\n",
    "       1. [Implement a handler service](#Implement-a-handler-service)       \n",
    "       1. [Implement an entrypoint](#Implement-an-entrypoint)              \n",
    "    1. [The Dockerfile](#The-Dockerfile)\n",
    "1. [Part 2: Building and registering the container](#Part-2:-Building-and-registering-the-container)\n",
    "1. [Part 3: Use the container for inference in Amazon SageMaker](#Part-3:-Use-the-container-for-inference-in-Amazon-SageMaker)\n",
    "  1. [Import model into hosting](#Import-model-into-hosting)\n",
    "  1. [Create endpoint configuration](#Create-endpoint-configuration) \n",
    "  1. [Create endpoint](#Create-endpoint)   \n",
    "  1. [Invoke model](#Invoke-model)     \n",
    "1. [(Optional) cleanup](#(Optional)-cleanup)  \n",
    "\n",
    "## Part 1: Packaging your code for inference with Amazon SageMaker\n",
    "\n",
    "### How Amazon SageMaker runs your Docker container during hosting\n",
    "\n",
    "Because you can run the same image in training or hosting, Amazon SageMaker runs your container with the argument `train` or `serve`. How your container processes this argument depends on the container. All SageMaker framework containers already cover this requirement and will trigger your defined training algorithm and inference code.\n",
    "\n",
    "* If you specify a program as an `ENTRYPOINT` in the Dockerfile, that program will be run at startup and its first argument will be `train` or `serve`. The program can then look at that argument and decide what to do.\n",
    "\n",
    "#### Running your container during hosting\n",
    "\n",
    "Hosting has a very different model than training because hosting is reponding to inference requests that come in via HTTP. \n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` receives `GET` requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these are passed in as well. \n",
    "\n",
    "If you are using the same container image for both training and serving the model, it will have the model files in the same place that they were written to during training:\n",
    "\n",
    "    /opt/ml\n",
    "    `-- model\n",
    "        `-- <model files>\n",
    "        \n",
    "Alternatively, if you are using separate containers for training and inference, when the inference container is spun up, the model files will be copied from the S3 location that the training container outputted them to. \n",
    "\n",
    "### The parts of the sample inference container\n",
    "\n",
    "The `inference_container` directory has all the components you need to extend the inference logic of the SageMaker scikit-learn container:\n",
    "\n",
    "    .\n",
    "    |-- Dockerfile\n",
    "    |-- handler_service.py\n",
    "    |-- serve.py\n",
    "\n",
    "Let's discuss each of these in turn:\n",
    "\n",
    "* __`Dockerfile`__ describes how to build your Docker container image. More details are provided below.\n",
    "* __`handler_service.py`__ is the program that defines a handler service, together with an inference handler to load the model, pre-process input data, get predictions and output data\n",
    "* __`serve.py`__ is the entrypoint for the application\n",
    "\n",
    "In this simple application, we install only one file in the container. You may only need that many, but if you have many supporting routines, you may wish to install more.\n",
    "\n",
    "### Creating an inference handler\n",
    "\n",
    "The [SageMaker inference toolkit](https://github.com/aws/sagemaker-inference-toolkit) is built on the multi-model server (MMS). MMS expects a Python script that implements functions to load the model, pre-process input data, get predictions from the model, and process the output data in a model handler.\n",
    "\n",
    "#### The model_fn Function\n",
    "\n",
    "The model_fn function is responsible for loading your model. It takes a `model_dir` argument that specifies where the model is stored. How you load your model depends on the framework you are using. There is no default implementation for the `model_fn` function. You must implement it yourself. This is how the `model_fn` function that loads the LightFM model would look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(self, model_dir):\n",
    "    import pickle\n",
    "    \n",
    "    logger.info('Loading LightFM model...')\n",
    "    return pickle.load(open( \"model.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input_fn Function\n",
    "\n",
    "The `input_fn` function is responsible for deserializing your input data so that it can be passed to your model. It takes input data and content type as parameters, and returns deserialized data. The SageMaker inference toolkit provides a default implementation that deserializes the following content types:\n",
    "\n",
    "* JSON\n",
    "* CSV\n",
    "* Numpy array\n",
    "* NPZ\n",
    "\n",
    "If your model requires a different content type, or you want to preprocess your input data before sending it to the model, you must implement the `input_fn` function. The following example shows a simple implementation of the `input_fn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_inference import content_types, decoder\n",
    "def input_fn(self, input_data, content_type):\n",
    "        \"\"\"A default input_fn that can handle JSON, CSV and NPZ formats.\n",
    "         \n",
    "        Args:\n",
    "            input_data: the request payload serialized in the content_type format\n",
    "            content_type: the request content_type\n",
    "\n",
    "        Returns: JSON\n",
    "        \"\"\"\n",
    "        return decoder.decode(input_data, content_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The predict_fn Function\n",
    "\n",
    "The predict_fn function is responsible for getting predictions from the model. It takes the model and the data returned from input_fn as parameters, and returns the prediction. There is no default implementation for the predict_fn. You must implement it yourself. The following is a simple implementation of the predict_fn function for a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(self, data, model):\n",
    "        \"\"\"A default predict_fn for. Calls a model on data deserialized in input_fn.\n",
    "\n",
    "        Args:\n",
    "            data: input data (numpy array) for prediction deserialized by input_fn\n",
    "            model: LightFM model loaded in memory by model_fn\n",
    "\n",
    "        Returns: a prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        f: lambda x: model.predict(x)\n",
    "            \n",
    "        return f(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output_fn Function\n",
    "The output_fn function is responsible for serializing the data that the predict_fn function returns as a prediction. The SageMaker inference toolkit implements a default output_fn function that serializes Numpy arrays, JSON, and CSV. If your model outputs any other content type, or you want to perform other post-processing of your data before sending it to the user, you must implement your own output_fn function. The following shows a simple output_fn function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_inference import encoder\n",
    "def output_fn(self, prediction, accept):\n",
    "        \"\"\"A default output_fn. Serializes predictions from predict_fn to JSON, CSV or NPY format.\n",
    "\n",
    "        Args:\n",
    "            prediction: a prediction result from predict_fn\n",
    "            accept: type which the output data needs to be serialized\n",
    "\n",
    "        Returns: output data serialized\n",
    "        \"\"\"\n",
    "        return encoder.encode(prediction, accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a handler service\n",
    "The handler service is executed by the model server. The handler service implements initialize and handle methods. The initialize method is invoked when the model server starts, and the handle method is invoked for all incoming inference requests to the model server. For more information, see [Custom Service in the Multi-model server documentation](https://github.com/awslabs/multi-model-server/blob/master/docs/custom_service.md). The following is an example of a handler service for our model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_inference.default_handler_service import DefaultHandlerService\n",
    "from sagemaker_inference.transformer import Transformer\n",
    "from sagemaker_pytorch_serving_container.default_inference_handler import DefaultPytorchInferenceHandler\n",
    "\n",
    "\n",
    "class HandlerService(DefaultHandlerService):\n",
    "    \"\"\"Handler service that is executed by the model server.\n",
    "    Determines specific default inference handlers to use based on model being used.\n",
    "    This class extends ``DefaultHandlerService``, which define the following:\n",
    "        - The ``handle`` method is invoked for all incoming inference requests to the model server.\n",
    "        - The ``initialize`` method is invoked at model server start up.\n",
    "    Based on: https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        transformer = Transformer(default_inference_handler=DefaultPytorchInferenceHandler())\n",
    "        super(HandlerService, self).__init__(transformer=transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an entrypoint\n",
    "The entrypoint starts the model server by invoking the handler service. You specify the location of the entrypoint in your Dockerfile. The following is an example of an entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_inference import model_server\n",
    "\n",
    "model_server.start_model_server(handler_service=HANDLER_SERVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile\n",
    "In your Dockerfile, copy the model handler from step 2 and specify the Python file from the previous step as the entrypoint in your Dockerfile. The following is an example of the lines you can add to your Dockerfile to copy the model handler and specify the entrypoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the default custom service file to handle incoming data and inference requests\n",
    "COPY model_handler.py /home/model-server/model_handler.py\n",
    "\n",
    "# Define an entrypoint script for the docker image\n",
    "ENTRYPOINT [\"python\", \"/usr/local/bin/serve.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building and registering the container\n",
    "\n",
    "Just like with the training container, we are going to use the [Amazon SageMaker Studio Image Build new CLI](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/).\n",
    "\n",
    "Open a terminal window and run the following command:\n",
    "```\n",
    "cd ~/inference_container\n",
    "sm-docker build . --repository lightfm-inference:1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Use the container for inference in Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "def get_container_uri(ecr_repository, tag):\n",
    "    account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    uri_suffix = 'amazonaws.com'\n",
    "    if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "        uri_suffix = 'amazonaws.com.cn'\n",
    "\n",
    "    return '{}.dkr.ecr.{}.{}/{}:{}'.format(account_id, region, uri_suffix, ecr_repository, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model into hosting\n",
    "\n",
    "When creating the Model entity for endpoints, the container's ModelDataUrl is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.\n",
    "\n",
    "The Mode of container is specified as MultiModel to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role()\n",
    "client = boto3.client(service_name='sagemaker')\n",
    "\n",
    "byoc_image_uri = get_container_uri('lightfm-inference','1.0')\n",
    "model_url = 's3://sagemaker-us-east-1-718026778991/light-fm-custom-container-train-job-2021-04-26-10-26-53-215/output/model.tar.gz'\n",
    "model_name = 'Demo-LightFM-Inference-Model'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "container = {\n",
    "    'Image': byoc_image_uri,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'SingleModel'\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'DEMO-LightFM-EndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-LightFMEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke model\n",
    "\n",
    "Now we invoke the model that we uploaded to S3 previously in the training step. \n",
    "\n",
    "The first invocation of a model may be slow, since behind the scenes, SageMaker is downloading the model artifacts from S3 to the instance and loading it into the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "runtime_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "data = np.array([3, 42, 500])\n",
    "payload = json.dumps(data.tolist())\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "#    TargetModel='resnet_18.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) cleanup\n",
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account, but you can also run the code below to easily clean up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)\n",
    "client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
